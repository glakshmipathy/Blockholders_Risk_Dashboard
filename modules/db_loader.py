import pandas as pd
from neo4j import GraphDatabase
import os
from dotenv import load_dotenv
from gqlalchemy import Node, Relationship

# Load environment variables (from project root .env)
load_dotenv()

class DBLoader:
    """
    Handles all data loading and initial enrichment into Memgraph.
    Now loads blockholder data, enriched company metadata, and risk exposures
    from CSVs generated by data_enricher.py.
    """
    def __init__(self, uri=None, user=None, password=None):
        self.uri = uri or os.getenv("MEMGRAPH_URI")
        self.user = user or os.getenv("MEMGRAPH_USER")
        self.password = password or os.getenv("MEMGRAPH_PASSWORD")

        if not all([self.uri, self.user, self.password is not None]):
            raise ValueError("Memgraph connection details (URI, USER, PASSWORD) are required in .env file.")

        try:
            self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))
            self.driver.verify_connectivity()
            print(f"INFO: DBLoader connected to Memgraph at {self.uri}.")
        except Exception as e:
            print(f"ERROR: DBLoader failed to connect to Memgraph at {self.uri}. Ensure Memgraph is running. Error: {e}")
            raise

    def close(self):
        if self.driver:
            self.driver.close()
            print("INFO: DBLoader connection closed.")

    def load_blockholders(self, csv_file_path, chunk_size=10000, start_year=2020, end_year=2023):
        """
        Loads the Blockholder dataset into Memgraph, filtering by a specific year range.
        Creates/merges Blockholder and Company nodes, and OWNS relationships.
        """
        print(f"\n--- Starting to load Blockholder data from: {csv_file_path} (filtered for years {start_year}-{end_year}) ---")
        total_rows_processed = 0

        try:
            for i, chunk_df in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size)):
                print(f"INFO: Processing chunk {i+1} (approx. {len(chunk_df)} rows)...")

                chunk_df_processed = chunk_df.dropna(subset=['blockholder_CIK', 'company_CIK', 'year']).copy()
                if chunk_df_processed.empty:
                    print(f"WARNING: Chunk {i+1} empty after dropping NaNs. Skipping.")
                    continue

                chunk_df_processed['blockholder_CIK'] = chunk_df_processed['blockholder_CIK'].astype(str).str.strip()
                chunk_df_processed['company_CIK'] = chunk_df_processed['company_CIK'].astype(str).str.strip()

                chunk_df_processed["blockholder_id_graph"] = "B_" + chunk_df_processed["blockholder_CIK"]
                chunk_df_processed["company_id_graph"] = "C_" + chunk_df_processed["company_CIK"]

                chunk_df_processed["ownership_percent"] = pd.to_numeric(chunk_df_processed["position"], errors='coerce') / 100.0

                chunk_df_processed["year"] = pd.to_numeric(chunk_df_processed["year"], errors='coerce')
                chunk_df_processed = chunk_df_processed[chunk_df_processed["year"].notna()]
                chunk_df_processed["year"] = chunk_df_processed["year"].astype(int)

                # --- UPDATED: Filter by the new year range ---
                initial_rows_in_chunk = len(chunk_df_processed)
                chunk_df_processed = chunk_df_processed[
                    (chunk_df_processed['year'] >= start_year) & 
                    (chunk_df_processed['year'] <= end_year)
                ]
                if len(chunk_df_processed) < initial_rows_in_chunk:
                    print(f"INFO: Filtered out {initial_rows_in_chunk - len(chunk_df_processed)} rows from chunk {i+1} outside {start_year}-{end_year}.")

                records_to_load = chunk_df_processed[[
                    "blockholder_id_graph", "blockholder_name",
                    "company_id_graph", "company_name",
                    "ownership_percent", "year", "block_type", "files_13F"
                ]].to_dict(orient="records")

                if not records_to_load:
                    print(f"WARNING: No valid records to load in chunk {i+1} after preprocessing/filtering. Skipping.")
                    continue

                with self.driver.session() as session:
                    session.write_transaction(self._create_blockholder_ownership_batch, records_to_load)
                    total_rows_processed += len(records_to_load)
                    print(f"INFO: Successfully processed {len(records_to_load)} records in chunk {i+1}. Total rows loaded: {total_rows_processed}")
        except FileNotFoundError:
            print(f"ERROR: CSV file not found at: {csv_file_path}. Please check the path.")
            raise
        except Exception as e:
            print(f"FATAL ERROR: An unexpected error occurred during Blockholder CSV loading: {e}")
            raise

        print(f"--- Finished loading {total_rows_processed} Blockholder data (filtered). ---")

    def _create_blockholder_ownership_batch(self, tx, records):
        """
        Cypher query for batch creation of Blockholder and Company nodes and OWNS relationships.
        """
        cypher_query = """
            UNWIND $records AS row
            MERGE (b:Blockholder {id: row.blockholder_id_graph})
                SET b.name = row.blockholder_name,
                    b.type = row.block_type,
                    b.files_13F = toInteger(row.files_13F)
            MERGE (c:Company {id: row.company_id_graph})
                SET c.name = row.company_name
            MERGE (b)-[r:OWNS]->(c)
                SET r.percent = toFloat(row.ownership_percent),
                    r.year = toInteger(row.year)
        """
        tx.run(cypher_query, records=records)

    def load_enriched_company_metadata(self, csv_file_path: str):
        """
        Loads enriched company metadata (sector, location, volatility, market_cap)
        and updates existing Company nodes.
        """
        print(f"\n--- Loading enriched company metadata from: {csv_file_path} ---")
        try:
            metadata_df = pd.read_csv(csv_file_path)
            records_to_update = metadata_df.to_dict(orient="records")

            if not records_to_update:
                print("WARNING: Enriched company metadata CSV is empty. Skipping update.")
                return

            cypher_query = """
                UNWIND $records AS row
                MATCH (c:Company {id: row.company_id_graph})
                SET c.sector = row.sector,
                    c.location = row.location,
                    c.volatility = toFloat(row.volatility)
            """
            chunk_size = 5000
            for i in range(0, len(records_to_update), chunk_size):
                chunk = records_to_update[i:i + chunk_size]
                with self.driver.session() as session:
                    session.write_transaction(lambda tx: tx.run(cypher_query, records=chunk))
                print(f"INFO: Updated {len(chunk)} Company nodes with enriched metadata in chunk.")

            print(f"INFO: Finished updating {len(records_to_update)} Company nodes with enriched metadata.")
        except FileNotFoundError:
            print(f"ERROR: Enriched company metadata CSV not found at: {csv_file_path}. Please run data_enricher.py first.")
            raise
        except Exception as e:
            print(f"FATAL ERROR: An unexpected error occurred during enriched company metadata loading: {e}")
            raise
        print("--- Finished loading enriched company metadata. ---")

    def load_risk_exposures_from_csv(self, csv_file_path: str):
        """
        Loads risk exposure relationships (EXPOSED_TO) from a CSV.
        """
        print(f"\n--- Loading EXPOSED_TO relationships from: {csv_file_path} ---")
        try:
            exposures_df = pd.read_csv(csv_file_path)
            records_to_load = exposures_df.to_dict(orient="records")

            if not records_to_load:
                print("WARNING: Risk exposures CSV is empty. Skipping EXPOSED_TO relationships.")
                return
            
            with self.driver.session() as session:
                session.run("MATCH ()-[e:EXPOSED_TO]->() DELETE e")
                print("INFO: Cleared existing EXPOSED_TO relationships.")

            cypher_query = """
                UNWIND $records AS row
                MATCH (c:Company {id: row.company_id})
                MERGE (r:RiskFactor {name: row.risk_factor})
                MERGE (c)-[e:EXPOSED_TO]->(r)
                SET e.weight = toFloat(row.risk_weight)
            """
            chunk_size = 5000
            for i in range(0, len(records_to_load), chunk_size):
                chunk = records_to_load[i:i + chunk_size]
                with self.driver.session() as session:
                    session.write_transaction(lambda tx: tx.run(cypher_query, records=chunk))
                print(f"INFO: Loaded {len(chunk)} risk exposures in chunk. Total loaded: {i + len(chunk)}")

        except FileNotFoundError:
            print(f"ERROR: Risk exposures CSV not found at: {csv_file_path}. Please run data_enricher.py first.")
            raise
        except Exception as e:
            print(f"FATAL ERROR: An unexpected error occurred during EXPOSED_TO loading: {e}")
            raise
        print("--- Finished loading EXPOSED_TO relationships. ---")

    def load_market_cap_data(self, csv_file_path: str):
        """
        Loads market cap data from a CSV and updates existing Company nodes.
        """
        print(f"\n--- Loading market capitalization data from: {csv_file_path} ---")
        try:
            market_cap_df = pd.read_csv(csv_file_path)
            records_to_update = market_cap_df.to_dict(orient="records")

            if not records_to_update:
                print("WARNING: Market cap CSV is empty. Skipping update.")
                return

            cypher_query = """
                UNWIND $records AS row
                MATCH (c:Company {id: row.company_id_graph})
                SET c.market_cap = toFloat(row.market_cap)
            """
            chunk_size = 5000
            for i in range(0, len(records_to_update), chunk_size):
                chunk = records_to_update[i:i + chunk_size]
                with self.driver.session() as session:
                    session.write_transaction(lambda tx: tx.run(cypher_query, records=chunk))
                print(f"INFO: Updated {len(chunk)} Company nodes with market cap data.")
            print(f"INFO: Finished updating {len(records_to_update)} Company nodes with market cap.")
        except FileNotFoundError:
            print(f"ERROR: Market cap CSV not found at: {csv_file_path}. Please run generate_market_cap.py first.")
            raise
        except Exception as e:
            print(f"FATAL ERROR: An unexpected error occurred during market cap loading: {e}")
            raise
        print("--- Finished loading market capitalization data. ---")

# --- Moved data model classes to this file ---
class Blockholder(Node):
    __label__ = "Blockholder"
    id: str
    name: str
    type: str = None
    files_13F: int = None
    total_risk: float = 0.0
    # NEW: Dollarized risk for tooltips
    dollarized_risk: float = 0.0

class Company(Node):
    __label__ = "Company"
    id: str
    name: str
    sector: str = None
    location: str = None
    volatility: float = None
    direct_risk: float = 0.0
    total_risk: float = 0.0
    # NEW: Market capitalization and dollarized risk
    market_cap: float = None
    dollarized_risk: float = 0.0
    role: str = None

class RiskFactor(Node):
    __label__ = "RiskFactor"
    name: str
    # NEW: Aggregated dollarized risk for visualization
    dollarized_risk: float = 0.0

class OWNS(Relationship):
    __type__ = "OWNS"
    percent: float = None
    year: int = None

class EXPOSED_TO(Relationship):
    __type__ = "EXPOSED_TO"
    weight: float = None